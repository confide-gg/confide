version: '3.8'

services:
  caddy:
    image: caddy:2-alpine
    ports:
      - target: 80
        published: 80
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        protocol: tcp
        mode: host
      - target: 443
        published: 443
        protocol: udp
        mode: host
    environment:
      - DOMAIN=${DOMAIN}
    configs:
      - source: caddy_config
        target: /etc/caddy/Caddyfile
    volumes:
      - caddy_data:/data
      - caddy_config_vol:/config
    networks:
      - confide-central
      - public
    deploy:
      mode: replicated
      replicas: 2
      placement:
        constraints:
          - node.role == worker
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:2019/metrics"]
      interval: 30s
      timeout: 5s
      start_period: 10s
      retries: 3

  central:
    image: ghcr.io/confide-gg/confide/central:${CENTRAL_VERSION:-latest}
    environment:
      - DATABASE_URL=postgres://confide:${POSTGRES_PASSWORD}@postgres:5432/confide_central
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - RUST_LOG=${RUST_LOG:-info,confide_central=info}
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=3000
      - CALLS_RELAY_BIND_HOST=0.0.0.0
      - CALLS_RELAY_BIND_PORT=10000
      - CALLS_RELAY_PUBLIC_HOST=${DOMAIN}
      - DB_MAX_CONNECTIONS=${DB_MAX_CONNECTIONS:-100}
      - DB_API_POOL_SIZE=${DB_API_POOL_SIZE:-80}
      - DB_WEBSOCKET_POOL_SIZE=${DB_WEBSOCKET_POOL_SIZE:-60}
      - WEBSOCKET_MESSAGE_BUFFER_SIZE=${WEBSOCKET_MESSAGE_BUFFER_SIZE:-256}
    ports:
      - target: 10000
        published: 10000
        protocol: udp
        mode: host
    networks:
      - confide-central
    deploy:
      mode: replicated
      replicas: ${CENTRAL_REPLICAS:-3}
      update_config:
        parallelism: 1
        delay: 10s
        order: start-first
        failure_action: rollback
      rollback_config:
        parallelism: 1
        delay: 5s
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '1'
          memory: 1G
      placement:
        constraints:
          - node.role == worker
        preferences:
          - spread: node.labels.zone
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      start_period: 30s
      retries: 3

  postgres:
    image: postgres:17.2-alpine
    environment:
      POSTGRES_USER: confide
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: confide_central
      POSTGRES_INITDB_ARGS: "-E UTF8 --locale=C"
      POSTGRES_MAX_CONNECTIONS: ${POSTGRES_MAX_CONNECTIONS:-200}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    networks:
      - confide-central
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.postgres == true
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U confide -d confide_central"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    command:
      - "postgres"
      - "-c"
      - "max_connections=${POSTGRES_MAX_CONNECTIONS:-200}"
      - "-c"
      - "shared_buffers=2GB"
      - "-c"
      - "effective_cache_size=6GB"
      - "-c"
      - "maintenance_work_mem=512MB"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=16MB"
      - "-c"
      - "default_statistics_target=100"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "work_mem=5242kB"
      - "-c"
      - "min_wal_size=1GB"
      - "-c"
      - "max_wal_size=4GB"

  redis:
    image: redis:7.4-alpine
    command:
      - "redis-server"
      - "--requirepass"
      - "${REDIS_PASSWORD}"
      - "--maxmemory"
      - "${REDIS_MAX_MEMORY:-2gb}"
      - "--maxmemory-policy"
      - "allkeys-lru"
      - "--appendonly"
      - "yes"
      - "--appendfsync"
      - "everysec"
      - "--save"
      - "900 1"
      - "--save"
      - "300 10"
      - "--save"
      - "60 10000"
      - "--tcp-backlog"
      - "511"
      - "--timeout"
      - "300"
      - "--tcp-keepalive"
      - "60"
      - "--maxclients"
      - "10000"
    volumes:
      - redis_data:/data
    networks:
      - confide-central
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.redis == true
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
      resources:
        limits:
          cpus: '2'
          memory: 2.5G
        reservations:
          cpus: '1'
          memory: 2G
    healthcheck:
      test: ["CMD", "redis-cli", "--pass", "${REDIS_PASSWORD}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

configs:
  caddy_config:
    file: ./config/Caddyfile

volumes:
  postgres_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /mnt/data/postgres
  redis_data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: /mnt/data/redis
  caddy_data:
    driver: local
  caddy_config_vol:
    driver: local

networks:
  confide-central:
    driver: overlay
    attachable: true
    ipam:
      config:
        - subnet: 10.0.1.0/24
  public:
    driver: overlay
    attachable: true
